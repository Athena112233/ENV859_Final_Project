{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blue Rockfish Habitat Predictions\n",
    "\n",
    "This notebook provides and training and prediction pipeline for habitat prediction. Please run the codes in order to properly train and make predictions. Outputs from this notebook will be saved to the scratch folder. Please add the final_predictions.shp file to arcgis pro to view the prediction. Prediction results are saved in the field \"habitat_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geopandas in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (0.10.2)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from geopandas) (1.8.22)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from geopandas) (1.2.3)\n",
      "Requirement already satisfied: shapely>=1.6 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from geopandas) (2.0.0)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from geopandas) (3.2.1)\n",
      "Requirement already satisfied: click>=4.0 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from fiona>=1.8->geopandas) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: munch in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: attrs>=17 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
      "Requirement already satisfied: certifi in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
      "Requirement already satisfied: six>=1.7 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from pandas>=0.25.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages\\pytz-2020.1-py3.7.egg (from pandas>=0.25.0->geopandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from pandas>=0.25.0->geopandas) (1.20.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn) (1.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: simpledbf in c:\\users\\al512\\appdata\\roaming\\python\\python37\\site-packages (0.2.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install geopandas \n",
    "! pip install xgboost \n",
    "! pip install scikit-learn \n",
    "! pip install simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pickle\n",
    "import xgboost\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from simpledbf import Dbf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_train_data(files):\n",
    "    \"\"\"\n",
    "    Preprocess training data. Extract environmental information from the raster layers at observed and random points.\n",
    "    Return sampled data file paths and their corresponding training labels (0 or 1).\n",
    "    \n",
    "    param files: a list of file paths. First path should be observed points, \n",
    "                 and the second path should be random points. The remanining paths should be 3-15 layers of rasters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up env\n",
    "    arcpy.env.workspace = '../../data'\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    save_path = '../../scratch/'\n",
    "    \n",
    "    # gather the shape files\n",
    "    files = np.array(files)\n",
    "    obs = files[0]\n",
    "    rnd = files[1]\n",
    "    \n",
    "    # gather the raster files\n",
    "    rasters = files[2:]\n",
    "    rasters = list(rasters[rasters != '#'])\n",
    "\n",
    "    # sample rasters from observed points\n",
    "    obs_table = save_path + 'observed_presence_sampled.dbf'\n",
    "    obs_points = arcpy.sa.Sample(rasters, obs, obs_table)\n",
    "    obs_count = int(arcpy.management.GetCount(obs_table)[0])\n",
    "    obs_label = np.array([1]*obs_count)\n",
    "    \n",
    "    # sample rasters from absence points\n",
    "    rnd_table = save_path + 'random_absence_sampled.dbf'\n",
    "    rnd_points = arcpy.sa.Sample(rasters, rnd, rnd_table)\n",
    "    rnd_count = int(arcpy.management.GetCount(rnd_table)[0])\n",
    "    rnd_label = np.array([0]*rnd_count)\n",
    "\n",
    "    return obs_table, obs_label, rnd_table, rnd_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(files):\n",
    "    \"\"\"\n",
    "    Build xgboost model with sampled points. Perform k-fold corss validation to find the best performing model.\n",
    "    Output trained model in a pickle file and save to the scratch folders\n",
    "    \n",
    "    param files: a list of file paths. First path should be observed points, \n",
    "                 and the second path should be random points. The remanining paths should be 3-15 layers of rasters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess data\n",
    "    obs_path, obs_label, rnd_path, rnd_label = preprocess_train_data(files)\n",
    "    \n",
    "    # read training file\n",
    "    obs_dbf = Dbf5(obs_path)\n",
    "    rnd_dbf = Dbf5(rnd_path)\n",
    "    obs_df = obs_dbf.to_dataframe()\n",
    "    rnd_df = rnd_dbf.to_dataframe()\n",
    "\n",
    "    # prepare data for training\n",
    "    obs_df = obs_df.drop(columns = ['brf_obs', 'X', 'Y'])\n",
    "    rnd_df = rnd_df.drop(columns = ['rand_obs', 'X', 'Y'])\n",
    "    obs_df['label'] = obs_label\n",
    "    rnd_df['label'] = rnd_label\n",
    "    data = pd.concat([obs_df, rnd_df])\n",
    "    X = data.drop(columns = ['label'])\n",
    "    Y = data['label']\n",
    "    \n",
    "    # build xgboost model\n",
    "    clf = XGBClassifier(objective= 'binary:logistic')\n",
    "    parameters = {\n",
    "        'max_depth': range (2, 10),\n",
    "        'n_estimators': range(60, 220, 40),\n",
    "        'learning_rate': [0.1, 0.01, 0.05]\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = clf,\n",
    "        param_grid = parameters,\n",
    "        scoring = 'roc_auc',\n",
    "        n_jobs = 5,\n",
    "        cv = 5,\n",
    "        verbose = True)\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    # print accuracy of model\n",
    "    acc = sklearn.metrics.accuracy_score(grid_search.predict(X), Y)\n",
    "    print('Training Accuracy is: ' + str(acc))\n",
    "    \n",
    "    # save model to scratch folder\n",
    "    model_path = \"../../scratch/xgb_model.pkl\"\n",
    "    pickle.dump(grid_search, open(model_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eval_data(files):\n",
    "    \"\"\"\n",
    "    Preprocess prediction data. Extract environmental information from the raster layers at randomly generated points. \n",
    "    The number of generated points could be modified below. Return shape file with habitat prediction to scratch folder.\n",
    "    \n",
    "    param files: a list of file paths. The list should contain 3-15 layers of rasters.\n",
    "    \"\"\"\n",
    "    # define env\n",
    "    arcpy.env.workspace = 'scratch'\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "    # gather the raster files\n",
    "    files = np.array(files)\n",
    "    rasters = list(files[files != '#'])\n",
    "    \n",
    "    # create random points\n",
    "    points = arcpy.management.CreateRandomPoints('../../scratch', \n",
    "                                        'prediction_points',  \n",
    "                                        constraining_extent = rasters[0], \n",
    "                                        number_of_points_or_field = 300)\n",
    "    \n",
    "    \n",
    "    # sample env information using the generated points\n",
    "    out_table = '../../scratch/prediction_sampled.dbf'\n",
    "    out_points = arcpy.sa.Sample(rasters, points, out_table)\n",
    "    out_count = int(arcpy.management.GetCount(out_table)[0])\n",
    "    out_label = np.array([1]*out_count)\n",
    "    \n",
    "    return out_table, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(files):\n",
    "    \"\"\"\n",
    "    This methods would generate predictions based on the file inputs. \n",
    "    Output from this method would be automatically saved to the scratch folder\n",
    "    \n",
    "    \"\"\"\n",
    "    arcpy.env.workspace = 'sratch'\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "    # preprocess data\n",
    "    data, label = preprocess_eval_data(files)\n",
    "    dbf = Dbf5(data)\n",
    "    df = dbf.to_dataframe()\n",
    "    X = df.drop(columns = ['prediction', 'X', 'Y'])\n",
    "    \n",
    "    # load pre-trained model\n",
    "    model = pickle.load(open('../../scratch/xgb_model.pkl', 'rb'))\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Set local variables\n",
    "    df['habitat_prediction'] = predictions\n",
    "    \n",
    "    # convert pd to gdf\n",
    "    final_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y))\n",
    "    \n",
    "    # save gpd as shp\n",
    "    final_gdf.to_file('../../scratch/final_predictions.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Training Accuracy is: 0.9844559585492227\n"
     ]
    }
   ],
   "source": [
    "training_input_files = ['V:\\\\ENV859_Final_Project_al512\\\\data\\\\brf_obs.shp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\rand_obs.shp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\bathy', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_1', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_2', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_3', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_4', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_5', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_6', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_7', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_8', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_kelp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_100m', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\botc10_8ws', '#', '#']\n",
    "build_model(training_input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n"
     ]
    }
   ],
   "source": [
    "pred_input_files = ['V:\\\\ENV859_Final_Project_al512\\\\data\\\\bathy', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_1', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_2', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_3',\n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_4', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_5', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_6', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_7', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_8', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_kelp', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_100m', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\botc10_8ws', '#', '#']\n",
    "\n",
    "make_predictions(pred_input_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
