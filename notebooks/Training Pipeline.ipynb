{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blue Rockfish Habitat Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! pip install geopandas \n",
    "! pip install xgboost \n",
    "! pip install scikit-learn \n",
    "! pip install simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pickle\n",
    "import xgboost\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from simpledbf import Dbf5\n",
    "#import rasterio as rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eval_data(files):\n",
    "    # define env\n",
    "    arcpy.env.workspace = '../../data'\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    save_path = '../../scratch/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_train_data(files):\n",
    "    \"\"\"\n",
    "    description\n",
    "    \n",
    "    param files: \n",
    "    \"\"\"\n",
    "    \n",
    "    # set up env\n",
    "    arcpy.env.workspace = '../../data'\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    save_path = '../../scratch/'\n",
    "    \n",
    "    # gather the shape files\n",
    "    files = np.array(files)\n",
    "    obs = files[0]\n",
    "    rnd = files[1]\n",
    "    \n",
    "    # gather the raster files\n",
    "    rasters = files[2:]\n",
    "    rasters = list(rasters[rasters != '#'])\n",
    "\n",
    "    # sample rasters from observed points\n",
    "    obs_table = save_path + 'observed_presence_sampled.dbf'\n",
    "    obs_points = arcpy.sa.Sample(rasters, obs, obs_table)\n",
    "    obs_count = int(arcpy.management.GetCount(obs_table)[0])\n",
    "    obs_label = np.array([1]*obs_count)\n",
    "    \n",
    "    # sample rasters from absence points\n",
    "    rnd_table = save_path + 'random_absence_sampled.dbf'\n",
    "    rnd_points = arcpy.sa.Sample(rasters, rnd, rnd_table)\n",
    "    rnd_count = int(arcpy.management.GetCount(rnd_table)[0])\n",
    "    rnd_label = np.array([0]*rnd_count)\n",
    "\n",
    "    return obs_table, obs_label, rnd_table, rnd_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(files):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    \n",
    "    param files: \n",
    "    \"\"\"\n",
    "    # preprocess data\n",
    "    obs_path, obs_label, rnd_path, rnd_label = preprocess_train_data(files)\n",
    "    \n",
    "    # read training file\n",
    "    obs_dbf = Dbf5(obs_path)\n",
    "    rnd_dbf = Dbf5(rnd_path)\n",
    "    obs_df = obs_dbf.to_dataframe()\n",
    "    rnd_df = rnd_dbf.to_dataframe()\n",
    "\n",
    "    # prepare data for training\n",
    "    obs_df = obs_df.drop(columns = ['brf_obs', 'X', 'Y'])\n",
    "    rnd_df = rnd_df.drop(columns = ['rand_obs', 'X', 'Y'])\n",
    "    obs_df['label'] = obs_label\n",
    "    rnd_df['label'] = rnd_label\n",
    "    data = pd.concat([obs_df, rnd_df])\n",
    "    X = data.drop(columns = ['label'])\n",
    "    Y = data['label']\n",
    "    \n",
    "    # build xgboost model\n",
    "    clf = XGBClassifier(objective= 'binary:logistic')\n",
    "    parameters = {\n",
    "        'max_depth': range (2, 10),\n",
    "        'n_estimators': range(60, 220, 40),\n",
    "        'learning_rate': [0.1, 0.01, 0.05]\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = clf,\n",
    "        param_grid = parameters,\n",
    "        scoring = 'roc_auc',\n",
    "        n_jobs = 5,\n",
    "        cv = 5,\n",
    "        verbose = True)\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(grid_search.predict(X), Y)\n",
    "    print('Training Accuracy is: ' + str(acc))\n",
    "    \n",
    "    model_path = \"../../scratch/xgb_model.pkl\"\n",
    "    pickle.dump(grid_search, open(model_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(files):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    \n",
    "    param files\n",
    "    \"\"\"\n",
    "    files = files[0]\n",
    "    \n",
    "    # process files for prediction\n",
    "    data = preprocess_eval_data(files)\n",
    "    \n",
    "    # read model\n",
    "    model = pickle.load(open('../../scratch/xgb_model.pkl', 'rb'))\n",
    "    \n",
    "    # save raster file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_files = [ 'V:\\\\ENV859_Final_Project_al512\\\\data\\\\brf_obs.shp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\rand_obs.shp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\bathy', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_1', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_2', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_3', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_4', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_5', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_6', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_7', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_8', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_kelp', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_100m', \n",
    "                        'V:\\\\ENV859_Final_Project_al512\\\\data\\\\botc10_8ws', '#', '#']\n",
    "build_model(training_input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_files = ['V:\\\\ENV859_Final_Project_al512\\\\data\\\\bathy', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_1', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_2', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_3', ss\n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_4', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_5', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_6', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_7', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\habras10_8', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_kelp', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\dist_100m', \n",
    "                    'V:\\\\ENV859_Final_Project_al512\\\\data\\\\botc10_8ws', '#', '#']\n",
    "raster = make_predictions(pred_input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
